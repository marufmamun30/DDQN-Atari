{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from collections import namedtuple, deque\n",
    "from datetime import datetime as d\n",
    "import cProfile\n",
    "import pstats\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.autograd\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import AtariPreprocessing, RecordVideo, FrameStack\n",
    "from stable_baselines3.common.atari_wrappers import (\n",
    "    ClipRewardEnv,\n",
    "    EpisodicLifeEnv,\n",
    "    FireResetEnv,\n",
    "    MaxAndSkipEnv,\n",
    "    NoopResetEnv,\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "date = d.now()\n",
    "date = date.strftime(\"%Y-%m-%d-%H-%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = \"PongNoFrameskip-v4\"  # Select environment here\n",
    "writer = SummaryWriter(f\"Tensorboard_experiments/Atari/dqn-{env_id}-experiments-{date}\")\n",
    "run_name = f\"dqn_{env_id}_videos_{date}\"\n",
    "seed = 42\n",
    "\n",
    "env = gym.make(env_id, render_mode=\"rgb_array\", repeat_action_probability=0)\n",
    "env = gym.wrappers.RecordVideo(\n",
    "    env, episode_trigger=lambda x: x % 20 == 0, video_folder=f\"RL_Videos/Atari/{run_name}\", disable_logger=True\n",
    ")\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "env = NoopResetEnv(env, noop_max=30)\n",
    "env = MaxAndSkipEnv(env, skip=4)\n",
    "env = EpisodicLifeEnv(env)\n",
    "if \"FIRE\" in env.unwrapped.get_action_meanings():\n",
    "    env = FireResetEnv(env)\n",
    "\n",
    "env = ClipRewardEnv(env)\n",
    "env = gym.wrappers.ResizeObservation(env, (84, 84))\n",
    "env = gym.wrappers.GrayScaleObservation(env)\n",
    "env = gym.wrappers.FrameStack(env, 4)\n",
    "env.action_space.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create environment\n",
    "# env_spec = \"PongNoFrameskip-v4\"\n",
    "# env = gym.make(env_spec, obs_type=\"grayscale\", render_mode=\"rgb_array\")\n",
    "# # Apply some wrappers\n",
    "# env = AtariPreprocessing(env, screen_size=84, grayscale_obs=True, grayscale_newaxis=False)\n",
    "# env = FrameStack(env, num_stack=4)\n",
    "# env = RecordVideo(env, episode_trigger=lambda x: x % 10 == 0, video_folder=\"ddqn_pong_videos_4-13-24-6-00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Replay Buffer class\n",
    "Transition = namedtuple(\"Transition\", (\"state\", \"action\", \"next_state\", \"reward\", \"done\"))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        initial_epsilon: float,\n",
    "        epsilon_decay: float,\n",
    "        final_epsilon: float,\n",
    "    ):\n",
    "        \"\"\"Initialize a DQN RL agent, get action epsilon-greedily, and manage epsilon\n",
    "\n",
    "        Args:\n",
    "            initial_epsilon: The initial epsilon value\n",
    "            epsilon_decay: The decay for epsilon\n",
    "            final_epsilon: The final epsilon value\n",
    "            q_value_model: The DQN which outputs a Q-value for each of the two possible actions\n",
    "        \"\"\"\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.final_epsilon = final_epsilon\n",
    "        self.model = model\n",
    "\n",
    "    def get_action(self, obs) -> int:\n",
    "        \"\"\"\n",
    "        Returns the best action with probability (1 - epsilon)\n",
    "        otherwise a random action with probability epsilon to ensure exploration.\n",
    "        \"\"\"\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return env.action_space.sample()\n",
    "        else:\n",
    "            return int(torch.argmax(self.model(obs)))\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.final_epsilon, self.epsilon - self.epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# May need to adjust for environments other than CartPole\n",
    "state_shape = env.observation_space.shape\n",
    "possible_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(state_shape)\n",
    "print(possible_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, info = env.reset()\n",
    "\n",
    "for _ in range(1000):\n",
    "    action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    print(info[\"lives\"])\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    # Get the current memory allocation (in bytes) on the default GPU\n",
    "    allocated_memory = torch.cuda.memory_allocated()\n",
    "    print(f\"Memory Allocated: {allocated_memory} bytes\")\n",
    "\n",
    "    # Convert bytes to gigabytes\n",
    "    allocated_memory_mb = allocated_memory / (1024**3)\n",
    "    print(f\"Memory Allocated: {allocated_memory_mb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, possible_actions):\n",
    "        super().__init__()\n",
    "        # self.bnorm = torch.nn.BatchNorm2d(num_features=4)\n",
    "        self.conv1 = nn.Conv2d(4, 32, 8, stride=4)\n",
    "        init.kaiming_normal_(self.conv1.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "        self.conv2 = nn.Conv2d(32, 64, 4, stride=2)\n",
    "        init.kaiming_normal_(self.conv2.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "        self.conv3 = nn.Conv2d(64, 64, 3, stride=1)\n",
    "        init.kaiming_normal_(self.conv3.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(3136, 512)\n",
    "        init.kaiming_normal_(self.fc1.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "        self.fc2 = nn.Linear(512, possible_actions)\n",
    "        init.normal_(self.fc2.weight, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.bnorm(x)\n",
    "        x = x / 255.0\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate policy and target models\n",
    "policy_model = DQN(possible_actions=possible_actions)\n",
    "policy_model.to(device)\n",
    "target_model = DQN(possible_actions=possible_actions)\n",
    "target_model.to(device)\n",
    "target_model.load_state_dict(policy_model.state_dict())\n",
    "# Freeze target model; we will update the target model gradually from the policy model during training\n",
    "for param in target_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create/reset the replay buffer\n",
    "buffer_size = 40_000\n",
    "replay_buffer = ReplayMemory(buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(replay_buffer.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "tau = 0.001\n",
    "\n",
    "learn_start = 40_000\n",
    "num_episodes = 100_000\n",
    "batch_size = 32\n",
    "final_exploration_frame = 750_000\n",
    "target_update_freq = 1000\n",
    "\n",
    "initial_epsilon = 1.0\n",
    "epsilon_decay = initial_epsilon / final_exploration_frame  # Reach final epsilon at 50% of num_episodes\n",
    "final_epsilon = 0.1\n",
    "\n",
    "discount_factor = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.HuberLoss(delta=1)\n",
    "optimizer = torch.optim.Adam(params=policy_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to render and observe model, significantly slows down training\n",
    "env.close()\n",
    "env = gym.make(env_id, render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = np.zeros(num_episodes)\n",
    "moving_average_window = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epsilon-greedy policy, used for training\n",
    "agent = DQNAgent(\n",
    "    model=policy_model,\n",
    "    initial_epsilon=initial_epsilon,\n",
    "    epsilon_decay=epsilon_decay,\n",
    "    final_epsilon=final_epsilon,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_frames = 0\n",
    "\n",
    "policy_model.train()\n",
    "for episode in range(num_episodes):\n",
    "    done = False\n",
    "    current_state, _ = env.reset()\n",
    "    current_state = torch.tensor(np.array(current_state, dtype=np.single), device=device).unsqueeze(0)\n",
    "    episode_return = 0\n",
    "    while not done:\n",
    "        action = agent.get_action(current_state)\n",
    "\n",
    "\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "\n",
    "        next_state = torch.tensor(np.array(next_state, dtype=np.single), device=device).unsqueeze(0)\n",
    "\n",
    "\n",
    "        replay_buffer.push(current_state, action, next_state, reward, done)\n",
    "        current_state = next_state\n",
    "        num_frames += 1\n",
    "\n",
    "        if num_frames < learn_start:\n",
    "            continue\n",
    "\n",
    "\n",
    "        transitions = replay_buffer.sample(batch_size=batch_size)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        reward_array = np.array(batch.reward, dtype=np.float32)\n",
    "        batch_states = torch.cat(batch.state)\n",
    "        batch_rewards = torch.tensor(reward_array, device=device).unsqueeze(1)\n",
    "        batch_actions = torch.tensor(batch.action, device=device).unsqueeze(1)\n",
    "        batch_next_states = torch.cat(batch.next_state)\n",
    "        batch_done = torch.tensor(batch.done, device=device).unsqueeze(1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Compute return from target network\n",
    "            next_actions = policy_model(batch_next_states).argmax(dim=1).unsqueeze(1)\n",
    "            target_values = target_model(batch_next_states)\n",
    "            best_next_q_values = torch.gather(target_values, 1, next_actions)\n",
    "            target_q_values = batch_rewards + discount_factor * best_next_q_values * (~batch_done)\n",
    "\n",
    "        predicted_q_values = policy_model(batch_states)\n",
    "        predicted_q_values = torch.gather(predicted_q_values, 1, batch_actions)\n",
    "        loss = criterion(predicted_q_values, target_q_values)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Polyak update target network towards policy network\n",
    "        # target_model_state_dict = target_model.state_dict()\n",
    "        # model_state_dict = model.state_dict()\n",
    "        # for key in model_state_dict:\n",
    "\n",
    "        #     target_model_state_dict[key] = model_state_dict[key] * tau + target_model_state_dict[key] * (1 - tau)\n",
    "        # target_model.load_state_dict(target_model_state_dict)\n",
    "        if num_frames % target_update_freq == 0:\n",
    "            target_model.load_state_dict(policy_model.state_dict())\n",
    "\n",
    "        episode_return += reward\n",
    "        writer.add_scalar(\"Loss/Train\", loss.item(), num_frames)\n",
    "        writer.add_scalar(\"Epsilon/Train\", agent.epsilon, num_frames)\n",
    "        agent.decay_epsilon()\n",
    "\n",
    "    returns[episode] = episode_return\n",
    "    if num_frames >= learn_start:\n",
    "        writer.add_scalar(\"Return/Train\", episode_return, episode)\n",
    "\n",
    "\n",
    "        writer.add_scalar(\n",
    "            f\"Avg Return, Window {moving_average_window}/Train\",\n",
    "            np.mean(returns[episode - moving_average_window + 1 : episode + 1]),\n",
    "            episode,\n",
    "        )\n",
    "    if (episode + 1) % 100 == 0:\n",
    "        print(\n",
    "            f\"Ep: {episode+1} Average return: {np.mean(returns[episode-moving_average_window+1:episode+1])} Eps: {agent.epsilon:.4f}\"\n",
    "        )\n",
    "    writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greedy policy, used for evaluation\n",
    "agent = DQNAgent(\n",
    "    model=policy_model,\n",
    "    initial_epsilon=0,\n",
    "    epsilon_decay=0,\n",
    "    final_epsilon=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation\n",
    "\n",
    "# losses = np.zeros(50_000_000)\n",
    "num_episodes = 1000\n",
    "returns_test = np.zeros(num_episodes)\n",
    "\n",
    "policy_model.eval()\n",
    "for episode in range(num_episodes):\n",
    "    done = False\n",
    "    current_state, _ = env.reset()\n",
    "    current_state = torch.tensor(np.array(current_state, dtype=np.single), device=device).unsqueeze(0)\n",
    "    episode_return = 0\n",
    "    while not done:\n",
    "        action = agent.get_action(current_state)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        next_state = torch.tensor(np.array(next_state, dtype=np.single), device=device).unsqueeze(0)\n",
    "\n",
    "        current_state = next_state\n",
    "        num_frames += 1\n",
    "\n",
    "        # losses[num_frames] = loss.item()\n",
    "        episode_return += reward\n",
    "        agent.decay_epsilon()\n",
    "\n",
    "    writer.add_scalar(\"Return/Test\", episode_return, episode)\n",
    "    returns_test[episode] = episode_return\n",
    "    if episode >= 9:\n",
    "        writer.add_scalar(\"Avg Return/Test\", np.mean(returns_test[episode - 9 : episode + 1]), episode)\n",
    "    if (episode + 1) % 100 == 0:\n",
    "        print(f\"Ep: {episode+1} Average return: {np.mean(returns_test[episode-9:episode+1])} Eps: {agent.epsilon:.4f}\")\n",
    "    writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(num_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiler = cProfile.Profile()\n",
    "profiler.enable()\n",
    "# optimize_model()\n",
    "profiler.disable()\n",
    "\n",
    "stats = pstats.Stats(profiler)\n",
    "stats.sort_stats(\"time\")\n",
    "stats.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model parameters and other info\n",
    "checkpoint_path = f\"Model_Checkpoints/Atari/dqn_{env_id}_{date}.ckpt\"\n",
    "additional_info = {\n",
    "    \"model_state_dict\": policy_model.state_dict(),\n",
    "    \"target_state_dict\": target_model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "}\n",
    "torch.save(additional_info, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and optimizer parameters\n",
    "checkpoint_path = \"\"\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "policy_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "target_model.load_state_dict(checkpoint[\"target_state_dict\"])\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transitions = replay_buffer.sample(batch_size=batch_size)\n",
    "test_batch = Transition(*zip(*test_transitions))\n",
    "\n",
    "test_batch_states = torch.cat(test_batch.state)\n",
    "test_batch_rewards = torch.tensor(test_batch.reward, device=device).unsqueeze(1)\n",
    "test_batch_actions = torch.tensor(test_batch.action, device=device).unsqueeze(1)\n",
    "test_batch_next_states = torch.cat(test_batch.next_state)\n",
    "test_batch_done = torch.tensor(test_batch.done, device=device).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"current_state.shape{current_state.shape}\")\n",
    "print(f\"next_state.shape{next_state.shape}\")\n",
    "print(f\"reward{reward}\")\n",
    "print(f\"done{done}\")\n",
    "print(f\"batch_states.shape{batch_states.shape}\")\n",
    "print(f\"batch_rewards{batch_rewards}\")\n",
    "print(f\"batch_actions{batch_actions}\")\n",
    "print(f\"batch_next_states.shape{batch_next_states.shape}\")\n",
    "print(f\"batch_done{batch_done}\")\n",
    "print(f\"next_actions{next_actions}\")\n",
    "print(f\"target_values{target_values}\")\n",
    "print(f\"best_next_q_values{best_next_q_values}\")\n",
    "print(f\"batch.done{batch.done}\")\n",
    "print(f\"batch_done{batch_done}\")\n",
    "print(f\"~batch_done{~batch_done}\")\n",
    "print(f\"target_q_values{target_q_values}\")\n",
    "print(f\"predicted_q_values{predicted_q_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(example):\n",
    "    img = example\n",
    "    plt.imshow(img.numpy(), cmap=\"gray\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images_grid(states, next_states):\n",
    "    fig, axs = plt.subplots(2, 4, figsize=(15, 6))  # Create a 2x4 grid of subplots\n",
    "\n",
    "    # Iterate over the first four states and plot them\n",
    "    for i in range(4):\n",
    "        img = states[0][i].cpu().numpy()  # Assuming states is a tensor\n",
    "        axs[0, i].imshow(img, cmap=\"gray\")  # Plot on row 1\n",
    "        axs[0, i].axis(\"off\")  # Turn off axis\n",
    "\n",
    "    # Iterate over the first four next states and plot them\n",
    "    for i in range(4):\n",
    "        img = next_states[0][i].cpu().numpy()  # Assuming next_states is a tensor\n",
    "        axs[1, i].imshow(img, cmap=\"gray\")  # Plot on row 2\n",
    "        axs[1, i].axis(\"off\")  # Turn off axis\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the states and next_states from your replay buffer\n",
    "view_states = replay_buffer.memory[749].state\n",
    "view_next_states = replay_buffer.memory[749].next_state\n",
    "\n",
    "# Call the function with states and next states\n",
    "plot_images_grid(view_states, view_next_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the states and next_states from your replay buffer\n",
    "view_states = replay_buffer.memory[750].state\n",
    "view_next_states = replay_buffer.memory[750].next_state\n",
    "\n",
    "# Call the function with states and next states\n",
    "plot_images_grid(view_states, view_next_states)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gymenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
